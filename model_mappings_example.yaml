# DeepFabric Model Mappings Configuration
# This file defines model-specific formatting rules for the HF Chat Template Formatter
#
# Usage:
#   deepfabric format --target-model MODEL_ID --model-config model_mappings.yaml

# Model family aliases for convenient grouping
model_families:
  qwen: ["Qwen/*", "qwen/*", "QWEN/*"]
  llama3: ["meta-llama/Llama-3*", "meta-llama/Meta-Llama-3*"]
  llama2: ["meta-llama/Llama-2*"]
  mistral: ["mistralai/Mistral-*", "mistralai/Mixtral-*"]
  deepseek: ["deepseek/*", "DeepSeek/*"]
  phi: ["microsoft/phi-*", "microsoft/Phi-*"]

# Model-specific configurations
models:
  # ============================================================================
  # QWEN MODELS - Native reasoning and tool support
  # ============================================================================

  Qwen/Qwen2.5-7B-Instruct:
    reasoning:
      start_tag: "<think>"
      end_tag: "</think>"
      native_support: true
      inject_mode: "native"  # Use <think> tags
      style: "compact"       # Remove extra whitespace
    tools:
      format: "xml"          # <tool_call> format
      start_tag: "<tool_call>"
      end_tag: "</tool_call>"
      response_tag: "<tool_response>"
      native_support: true

  # Pattern match for all Qwen models
  Qwen/*:
    reasoning:
      start_tag: "<think>"
      end_tag: "</think>"
      native_support: true
      inject_mode: "native"
    tools:
      format: "xml"
      start_tag: "<tool_call>"
      end_tag: "</tool_call>"
      response_tag: "<tool_response>"
      native_support: true

  # ============================================================================
  # LLAMA 3.1 - Native tool calls, inline reasoning
  # ============================================================================

  meta-llama/Llama-3.1-8B-Instruct:
    reasoning:
      inject_mode: "inline"  # No native <think> tags
      native_support: false
      prefix: ""             # No prefix, just inline text
      separator: "\n\n"      # Separate reasoning from answer
    tools:
      format: "native"       # Uses tool_calls field
      native_support: true

  # Pattern match for all Llama 3.1 variants
  meta-llama/Llama-3.1*:
    reasoning:
      inject_mode: "inline"
      native_support: false
    tools:
      format: "native"
      native_support: true

  # ============================================================================
  # LLAMA 3.2 - Similar to 3.1 but may have variants
  # ============================================================================

  meta-llama/Llama-3.2*:
    reasoning:
      inject_mode: "inline"
      native_support: false
    tools:
      format: "native"
      native_support: true

  # ============================================================================
  # DEEPSEEK - Custom reasoning tags
  # ============================================================================

  deepseek/deepseek-coder-33b-instruct:
    reasoning:
      start_tag: "<think>"
      end_tag: "</think>"
      native_support: true
      inject_mode: "native"
    tools:
      format: "xml"
      start_tag: "<tool_call>"
      end_tag: "</tool_call>"
      native_support: true

  # All DeepSeek models
  deepseek/*:
    reasoning:
      start_tag: "<think>"
      end_tag: "</think>"
      native_support: true
      inject_mode: "native"
    tools:
      format: "xml"
      native_support: true

  # ============================================================================
  # MISTRAL - Inline reasoning, native tools
  # ============================================================================

  mistralai/Mistral-7B-Instruct-v0.3:
    reasoning:
      inject_mode: "inline"
      native_support: false
      prefix: ""
    tools:
      format: "native"
      native_support: true

  mistralai/Mixtral-8x7B-Instruct-v0.1:
    reasoning:
      inject_mode: "inline"
      native_support: false
    tools:
      format: "native"
      native_support: true

  # Pattern for all Mistral models
  mistralai/*:
    reasoning:
      inject_mode: "inline"
      native_support: false
    tools:
      format: "native"
      native_support: true

  # ============================================================================
  # PHI MODELS - Microsoft's compact models
  # ============================================================================

  microsoft/phi-3-mini-4k-instruct:
    reasoning:
      inject_mode: "inline"
      native_support: false
      prefix: "Reasoning: "
      separator: "\n\n"
    tools:
      format: "native"
      native_support: true

  microsoft/phi-*:
    reasoning:
      inject_mode: "inline"
      native_support: false
    tools:
      format: "native"
      native_support: true

  # ============================================================================
  # CUSTOM EXAMPLES - Novel architectures
  # ============================================================================

  # Example: Model with "musing" reasoning format
  CustomLab/MusingModel-v1:
    reasoning:
      start_tag: "<musing>"
      end_tag: "</musing>"
      native_support: true
      inject_mode: "native"
      style: "verbose"       # Keep all whitespace
    tools:
      format: "custom_xml"
      start_tag: "<invoke>"
      end_tag: "</invoke>"
      response_tag: "<result>"
      native_support: false
    preprocessing:
      - type: "inject_marker"
        target: "system"
        marker: "[MUSING_MODE_ACTIVE]"
        position: "start"

  # Example: Model requiring special system message format
  AnotherOrg/SpecialModel-v2:
    reasoning:
      inject_mode: "inline"
      native_support: false
      prefix: "### Thinking Process:\n"
      separator: "\n\n### Answer:\n"
    tools:
      format: "native"
      native_support: true
    preprocessing:
      - type: "inject_marker"
        target: "system"
        marker: "[FUNCTION_CALLING_ENABLED]"
        position: "end"
      - type: "transform_content"
        target: "assistant"
        pattern: "I'll help"
        replacement: "I will assist"

  # Example: Model with "contemplation" reasoning
  ResearchLab/DeepThinkModel:
    reasoning:
      start_tag: "<contemplation>"
      end_tag: "</contemplation>"
      native_support: true
      inject_mode: "native"
    tools:
      format: "xml"
      start_tag: "<function_call>"
      end_tag: "</function_call>"
      response_tag: "<function_response>"
      native_support: true

  # Example: Model requiring numbered reasoning steps
  AcademicInstitute/StepByStepModel:
    reasoning:
      inject_mode: "structured"
      native_support: false
      format_template: "Step {step_number}: {thought}\nAction: {action}\n"
      separator: "\n---\n"
    tools:
      format: "native"
      native_support: true

# ============================================================================
# DEFAULT CONFIGURATIONS
# Applied when no specific model mapping is found
# ============================================================================

defaults:
  reasoning:
    inject_mode: "inline"       # Safe default: inline text
    native_support: false
    prefix: ""
    separator: "\n\n"
    style: "compact"
  tools:
    format: "native"            # Standard tool_calls field
    native_support: false
  preprocessing: []

# ============================================================================
# TRAINING FRAMEWORK CONFIGURATIONS
# Optional: Apply training-specific transformations
# ============================================================================

training_frameworks:
  # GRPO (Generalized Reward Preference Optimization)
  grpo:
    preprocessing:
      - type: "wrap_section"
        target: "assistant"
        section: "reasoning"
        start_tag: "<start_working_out>"
        end_tag: "<end_working_out>"
      - type: "wrap_section"
        target: "assistant"
        section: "final_answer"
        start_tag: "<SOLUTION>"
        end_tag: "</SOLUTION>"

  # DPO (Direct Preference Optimization)
  dpo:
    preprocessing:
      - type: "create_preference_pairs"
        chosen_field: "chosen"
        rejected_field: "rejected"

  # SFT (Supervised Fine-Tuning) - Standard
  sft:
    preprocessing: []  # No special processing

# ============================================================================
# CAPABILITY DETECTION RULES
# Automatic detection based on tokenizer special tokens
# ============================================================================

capability_detection:
  reasoning:
    # If tokenizer has these tokens, assume native support
    token_indicators:
      - "<think>"
      - "<thinking>"
      - "<thought>"
      - "<musing>"
      - "<contemplation>"

  tools:
    # If tokenizer has these, assume native tool support
    token_indicators:
      - "<tool_call>"
      - "<function_call>"
      - "<function>"
      - "<invoke>"

# ============================================================================
# RESOLUTION ORDER
# How model configurations are resolved (top to bottom)
# ============================================================================

# 1. Exact model ID match (e.g., "Qwen/Qwen2.5-7B-Instruct")
# 2. Glob pattern match (e.g., "Qwen/*", "meta-llama/Llama-3*")
# 3. Model family alias (e.g., "qwen", "llama3")
# 4. Automatic detection from tokenizer special tokens
# 5. Defaults (as specified above)

# ============================================================================
# NOTES AND BEST PRACTICES
# ============================================================================

# 1. Always test new model configurations with a small dataset first
# 2. Use exact model IDs for production deployments
# 3. Glob patterns are useful for model families with consistent behavior
# 4. Check HuggingFace model cards for official formatting guidelines
# 5. Contribute tested configurations back to the community!

# Example contributions:
# - Test with official model examples
# - Verify tool calling works correctly
# - Check reasoning format matches model expectations
# - Document any quirks or special requirements
