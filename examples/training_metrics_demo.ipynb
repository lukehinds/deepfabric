{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric Training Metrics Demo\n",
    "\n",
    "This notebook demonstrates automatic training metrics logging with DeepFabric.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, start a simple HTTP server to capture the metrics payloads. In a terminal, run:\n",
    "\n",
    "```bash\n",
    "# Option 1: Python (built-in, but only logs requests, not bodies)\n",
    "python -m http.server 8888\n",
    "\n",
    "# Option 2: npx http-echo-server (shows request bodies)\n",
    "npx http-echo-server 8888\n",
    "\n",
    "# Option 3: Python with request body logging (recommended)\n",
    "python examples/mock_metrics_server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set environment variables BEFORE importing deepfabric\nimport os\n\n# Point to our local test server (or your zrok URL for Colab testing)\nos.environ[\"DEEPFABRIC_API_URL\"] = \"http://localhost:8888\"\n\n# Optional: Set API key via env var (or be prompted interactively)\n# os.environ[\"DEEPFABRIC_API_KEY\"] = \"your-api-key\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training status\n",
    "!deepfabric training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)\n\nfrom deepfabric.training import DeepFabricCallback\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a tiny model for fast testing\n",
    "# sshleifer/tiny-gpt2 is only ~500KB!\n",
    "MODEL_NAME = \"sshleifer/tiny-gpt2\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny synthetic dataset\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"Deep learning models require large amounts of data.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Neural networks are inspired by the human brain.\",\n",
    "    \"Training large models requires significant compute resources.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific tasks.\",\n",
    "] * 10  # Repeat to have more samples\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - very short training for demo\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./demo_output\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=5,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=5,  # Log every 5 steps\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for demo\n",
    "    report_to=[],  # Disable default reporters (wandb, etc.)\n",
    "    # Use CPU for simplicity (change to \"cuda\" or \"mps\" if available)\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # Causal LM, not masked LM\n)\n\n# Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\n# Add DeepFabric callback for metrics logging\n# This will prompt for API key if not set in environment\ntrainer.add_callback(DeepFabricCallback(trainer))\n\n# Check what callbacks are registered\nprint(\"Registered callbacks:\")\nfor cb in trainer.callback_handler.callbacks:\n    print(f\"  - {type(cb).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "# Watch your HTTP server terminal - you should see metrics being sent\n",
    "print(\"Starting training...\")\n",
    "print(\"Check your HTTP server for incoming metrics!\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"./demo_output\", ignore_errors=True)\n",
    "print(\"Cleaned up demo output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Metrics\n",
    "\n",
    "You should see POST requests to your server with payloads like:\n",
    "\n",
    "### Run Start Event\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"run_start\",\n",
    "  \"run_id\": \"uuid-here\",\n",
    "  \"model_name\": \"sshleifer/tiny-gpt2\",\n",
    "  \"training_config\": {\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"learning_rate\": 0.0005\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Metrics Batch\n",
    "```json\n",
    "{\n",
    "  \"metrics\": [\n",
    "    {\n",
    "      \"run_id\": \"uuid-here\",\n",
    "      \"global_step\": 5,\n",
    "      \"epoch\": 0.25,\n",
    "      \"metrics\": {\n",
    "        \"loss\": 4.123,\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"grad_norm\": 1.234\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Run End Event\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"run_end\",\n",
    "  \"run_id\": \"uuid-here\",\n",
    "  \"final_step\": 40,\n",
    "  \"final_epoch\": 2.0\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}