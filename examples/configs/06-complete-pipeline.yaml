# ============================================================================
# COMPLETE PIPELINE - Full workflow from topics to upload
# ============================================================================
# Purpose: Comprehensive example showing the entire DeepFabric workflow
# Usage: deepfabric start examples/configs/06-complete-pipeline.yaml
# ============================================================================
#
# This config demonstrates:
# 1. Topic tree generation
# 2. Dataset generation with chain-of-thought reasoning
# 3. Multiple output formatters
# 4. HuggingFace Hub upload (optional)
#
# Perfect as a template for production use
# ============================================================================

# Define the overall purpose and behavior of your dataset
dataset_system_prompt: |
  You are an expert programming tutor specializing in Python and software engineering.
  Your responses should be:
  - Clear and educational
  - Include practical examples
  - Show step-by-step reasoning for complex topics
  - Appropriate for intermediate developers

# ============================================================================
# STEP 1: Topic Tree Generation
# ============================================================================
# Generate a hierarchical tree of related topics
# This creates diverse, organized training examples
# ============================================================================

topic_tree:
  # Root topic - this will be expanded into subtopics
  topic_prompt: |
    Advanced Python programming concepts including:
    - Async programming and concurrency
    - Meta-programming and decorators
    - Performance optimization
    - Design patterns
    - Testing and debugging

  # Model configuration for topic generation
  provider: "openai"
  model: "gpt-4o"  # Use GPT-4 for high-quality topic generation
  temperature: 0.8  # Higher temperature for creative topic exploration

  # Tree structure
  depth: 2     # 3 levels deep (root → categories → specific topics)
  degree: 2    # 3 branches per node

  # Save the topic tree for inspection
  save_as: "python_topics.jsonl"

# ============================================================================
# STEP 2: Dataset Generation
# ============================================================================
# Generate training examples based on the topic tree
# Using chain-of-thought for reasoning capabilities
# ============================================================================

data_engine:
  # System prompt for generation
  generation_system_prompt: |
    Generate high-quality Python programming Q&A with detailed explanations.
    Show your reasoning process when explaining complex concepts.
    Include code examples where appropriate.

  instructions: |
    Create educational programming examples that:
    - Teach practical skills
    - Include working code samples
    - Explain the "why" behind solutions
    - Are suitable for intermediate to advanced developers

  # Conversation configuration
  conversation_type: "chain_of_thought"
  reasoning_style: "hybrid"  # Both freetext analysis and structured steps

  # Model configuration for content generation
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7  # Balanced creativity and consistency

  # Generation settings
  max_retries: 3  # Retry on API failures
  max_tokens: 2048  # Allow longer responses for detailed explanations
  timeout: 120  # Request timeout in seconds

  # Save raw generated data
  save_as: "python_dataset_raw.jsonl"

# ============================================================================
# STEP 3: Dataset Configuration
# ============================================================================
# Configure dataset creation and formatting options
# ============================================================================

dataset:
  # Final dataset output path
  save_as: "python_dataset_final.jsonl"

  creation:
    # Generation parameters
    num_steps: 2      # Generate 8 examples
    batch_size: 2       # Process 2 at a time for efficiency

    # Additional model config for dataset creation
    sys_msg: true         # Include system message in output

  # ============================================================================
  # STEP 4: Formatters - Transform data for different use cases
  # ============================================================================
  # Apply multiple formatters to create different output versions
  #
  # NOTE: Some formatters may reject samples that don't match their expected format:
  # - alpaca: Works best with simple Q&A (not chain-of-thought or multi-turn)
  # - chatml: May reject samples with complex reasoning structures
  # - conversations: Most flexible, accepts all conversation types
  # - trl_sft_tools: Flexible, works with most conversation types
  # - xlam_v2: Requires agent_mode for tool-calling data
  #
  # DeepFabric will warn you about potential incompatibilities when loading config.
  # ============================================================================

  formatters:
    # Format 1: Standard conversation format (works with all conversation types)
    - name: "standard_training"
      template: "builtin://conversations"
      output: "python_dataset_standard.jsonl"
      config:
        include_system: true

    # Format 2: TRL SFT format for HuggingFace training (compatible with chain-of-thought)
    - name: "trl_sft"
      template: "builtin://trl_sft_tools"
      output: "python_dataset_trl.jsonl"
      config:
        include_system_in_user: false

    # Format 3: GRPO format for reasoning with rewards (optimized for chain-of-thought)
    - name: "grpo_format"
      template: "builtin://grpo"
      output: "python_dataset_grpo.jsonl"
      config:
        reasoning_start_tag: "<start_reasoning>"
        reasoning_end_tag: "</end_reasoning>"
        solution_start_tag: "<solution>"
        solution_end_tag: "</solution>"

# ============================================================================
# STEP 5: HuggingFace Hub Upload (Optional)
# ============================================================================
# Automatically upload your dataset to HuggingFace Hub
# Requires HF_TOKEN environment variable or huggingface-cli login
# ============================================================================

# huggingface:
#   # Repository name (format: username/dataset-name)
#   repository: "lukehinds/advanced-python-qa"

#   # Visibility (public or private)
#   private: false

#   # Dataset metadata
#   tags:
#     - "synthetic"
#     - "python"
#     - "programming"
#     - "chain-of-thought"
#     - "deepfabric"

#   # License
#   license: "mit"

  # Auto-generated dataset card will include:
  # - Dataset description
  # - Generation configuration
  # - Example samples
  # - Usage instructions

# ============================================================================
# ALTERNATIVE: Kaggle Upload (Optional)
# ============================================================================
# Uncomment to upload to Kaggle instead of (or in addition to) HuggingFace
# ============================================================================

# kaggle:
#   handle: "your-username/advanced-python-qa"
#   tags: ["synthetic", "python", "programming"]
#   description: "Advanced Python programming Q&A dataset with chain-of-thought reasoning"
#   version_notes: "Initial release - 100 examples covering advanced Python topics"

# ============================================================================
# RUNNING THIS PIPELINE:
#
# 1. Basic run:
#    deepfabric start examples/configs/06-complete-pipeline.yaml
#
# 2. Override specific parameters:
#    deepfabric start examples/configs/06-complete-pipeline.yaml \
#      --num-steps 200 \
#      --model gpt-4o-mini \
#      --temperature 0.8
#
# 3. Skip HuggingFace upload:
#    Comment out the 'huggingface:' section above
#
# 4. Use different provider:
#    deepfabric start examples/configs/06-complete-pipeline.yaml \
#      --provider anthropic \
#      --model claude-3-5-sonnet-20241022
#
# ============================================================================

# ============================================================================
# OUTPUT FILES:
#
# This pipeline will create:
# - python_topics.jsonl              (topic tree)
# - python_dataset_raw.jsonl         (raw generated data)
# - python_dataset_final.jsonl       (final dataset)
# - python_dataset_standard.jsonl    (conversations format - universal)
# - python_dataset_trl.jsonl         (TRL SFT Tools format - HuggingFace)
# - python_dataset_grpo.jsonl        (GRPO format - reasoning with rewards)
#
# Plus auto-generated dataset card if uploading to HuggingFace
# ============================================================================

# ============================================================================
# CUSTOMIZATION IDEAS:
#
# 1. Add custom tools for coding examples:
#    custom_tools:
#      - name: "code_executor"
#        description: "Execute Python code safely"
#        parameters: [...]
#      - name: "linter"
#        description: "Run linter on code"
#        parameters: [...]
#
# 2. Use agent mode for interactive coding help:
#    agent_mode: "single_turn"
#    available_tools: ["code_executor", "linter"]
#    max_tools_per_query: 3
#
# 3. Multi-turn agent for complex coding workflows:
#    agent_mode: "multi_turn"
#    min_turns: 2
#    max_turns: 6
#    available_tools: ["code_executor", "linter", "search_docs"]
#
# 4. Generate multi-format datasets (choose compatible formatters):
#    - conversations: Works with all conversation types (universal)
#    - trl_sft_tools: Works with all types, optimized for HuggingFace
#    - grpo: Best for chain-of-thought reasoning
#    - xlam_v2: For agent training (requires agent_mode)
#    - tool_calling: For embedded tool execution traces
#    - alpaca: Best for simple Q&A (avoid with chain-of-thought)
#    - chatml: Best for simple conversations (avoid with complex reasoning)
#
# 5. Integrate with CI/CD:
#    - Run pipeline on schedule
#    - Automatically upload new versions
#    - Version control your configs
# ============================================================================
