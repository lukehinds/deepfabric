{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric Quick Start\n",
    "\n",
    "**Goal:** Generate a small synthetic dataset and fine-tune a model in < 15 minutes\n",
    "\n",
    "**Perfect for:**\n",
    "- First-time users\n",
    "- Quick testing\n",
    "- Google Colab free tier\n",
    "- Limited GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepFabric\n",
    "!pip install 'deepfabric[training]' -q\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"❌ GPU not available! Enable GPU in runtime settings.\"\n",
    "print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfabric.pipeline import DeepFabricPipeline\n",
    "\n",
    "# Small, fast model for quick testing\n",
    "pipeline = DeepFabricPipeline(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    provider=\"transformers\",\n",
    "    device=\"cuda\",\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "# Generate 20 samples (takes ~3-5 minutes)\n",
    "# You can customize the generation_system_prompt to improve quality\n",
    "dataset = pipeline.generate_dataset(\n",
    "    topic_prompt=\"Python programming basics\",\n",
    "    num_samples=20,\n",
    "    batch_size=5,\n",
    "    tree_depth=2,\n",
    "    tree_degree=3,\n",
    "    generation_system_prompt=\"You are an expert Python educator creating clear, beginner-friendly training examples.\",\n",
    "    # Optional: add more parameters\n",
    "    # temperature=0.8,  # Higher = more creative\n",
    "    # instructions=\"Focus on practical, real-world examples\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {len(dataset)} samples\")\n",
    "dataset.save(\"quickstart_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfabric.training import SFTTrainingConfig, LoRAConfig\n",
    "\n",
    "# Quick training config\n",
    "config = SFTTrainingConfig(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    output_dir=\"./quickstart_model\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for quick test\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    lora=LoRAConfig(enabled=True, r=16),  # Memory efficient\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "# Train (takes ~5-10 minutes)\n",
    "metrics = pipeline.train(config)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./quickstart_model\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./quickstart_model\")\n",
    "\n",
    "# Test\n",
    "prompt = \"What is a Python list?\"\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=150, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"\\nA: {response.split('assistant')[-1].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Larger dataset**: Increase `num_samples` to 100-500\n",
    "2. **More epochs**: Try `num_train_epochs=3`\n",
    "3. **Different topics**: Change `topic_prompt`\n",
    "4. **Bigger model**: Use `\"Qwen/Qwen2.5-7B-Instruct\"` (needs more GPU memory)\n",
    "5. **Upload to Hub**: Set `push_to_hub=True` in config\n",
    "\n",
    "See `cuda_dataset_and_training.ipynb` for the complete workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
