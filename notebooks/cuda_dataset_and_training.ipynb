{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric: Dataset Generation + SFT Training on CUDA\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Generate a synthetic dataset using a local LLM\n",
    "2. Fine-tune a model on the generated dataset\n",
    "3. Save and optionally upload to HuggingFace Hub\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU with CUDA support\n",
    "- ~16GB GPU memory for 7B models\n",
    "- Python 3.11+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepFabric with training dependencies\n",
    "!pip install 'deepfabric[training]' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ CUDA not available! This notebook requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # or \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "TOPIC = \"Python Programming and Machine Learning\"\n",
    "NUM_SAMPLES = 200  # Number of training examples to generate\n",
    "DATASET_PATH = \"./synthetic_dataset.jsonl\"\n",
    "OUTPUT_DIR = \"./trained_model\"\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "USE_LORA = True  # Use LoRA for memory efficiency\n",
    "LORA_R = 32  # LoRA rank\n",
    "\n",
    "# Optional: HuggingFace Hub\n",
    "PUSH_TO_HUB = False\n",
    "HUB_MODEL_ID = \"your-username/your-model-name\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Topic: {TOPIC}\")\n",
    "print(f\"  Samples: {NUM_SAMPLES}\")\n",
    "print(f\"  LoRA: {USE_LORA}\")\n",
    "print(f\"  Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation\n",
    "\n",
    "Generate synthetic training data using the model on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfabric.pipeline import DeepFabricPipeline\n",
    "\n",
    "# Initialize pipeline with CUDA\n",
    "pipeline = DeepFabricPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    provider=\"transformers\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=\"bfloat16\",  # Use bfloat16 for better performance on modern GPUs\n",
    "    model_kwargs={\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding Generation Prompts\n\nDeepFabric uses two types of prompts:\n\n1. **`generation_system_prompt`**: Used during dataset generation to guide the LLM in creating training data\n   - Controls the quality, style, and focus of generated examples\n   - Should describe what kind of expert the LLM should be\n   - Should specify requirements for the training data\n\n2. **`dataset_system_prompt`**: (Optional) Included in the final dataset as the system message\n   - If not provided, defaults to `generation_system_prompt`\n   - Use when you want different behavior during generation vs. training\n\n**Additional Parameters:**\n- `instructions`: Extra instructions for data generation (e.g., \"Focus on edge cases\")\n- `temperature`: Controls creativity (0.0 = deterministic, 2.0 = very creative)\n- `max_tokens`: Maximum tokens per generated sample\n- `reasoning_style`: For CoT types - \"mathematical\", \"logical\", or \"general\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate dataset with custom generation prompts\nprint(f\"Generating {NUM_SAMPLES} synthetic training examples...\")\nprint(\"This may take several minutes depending on your GPU...\\n\")\n\n# Custom system prompt for generation (controls how the LLM generates data)\nGENERATION_SYSTEM_PROMPT = \"\"\"You are an expert AI training data generator specializing in Python programming and machine learning.\n\nYour task is to create high-quality, diverse training examples that:\n- Cover both fundamental concepts and advanced topics\n- Include practical, real-world scenarios\n- Demonstrate best practices and common patterns\n- Provide clear, accurate explanations\n- Use proper code examples when relevant\"\"\"\n\ndataset = pipeline.generate_dataset(\n    # Topic Configuration\n    topic_prompt=TOPIC,\n    tree_depth=3,  # How deep the topic tree goes (more depth = more specific topics)\n    tree_degree=5,  # How many subtopics per level (more = more variety)\n\n    # Generation Configuration\n    num_samples=NUM_SAMPLES,\n    batch_size=5,  # Process 5 samples at a time for efficiency\n    conversation_type=\"cot_structured\",  # Chain-of-thought with structured output\n\n    # Custom Prompts (optional but recommended for better quality)\n    generation_system_prompt=GENERATION_SYSTEM_PROMPT,  # Controls data generation quality\n    # dataset_system_prompt=None,  # Optional: different system prompt in final dataset\n\n    # Additional Options (uncomment to use)\n    # instructions=\"Focus on practical examples with code\",  # Additional instructions\n    # temperature=0.8,  # Higher = more creative (default 0.7)\n    # max_tokens=2000,  # Max tokens per sample (default 2000)\n    # reasoning_style=\"logical\",  # For CoT: \"mathematical\", \"logical\", or \"general\"\n)\n\nprint(f\"\\n✓ Generated {len(dataset)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "dataset.save(DATASET_PATH)\n",
    "print(f\"✓ Dataset saved to: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample conversation\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset.samples[0]\n",
    "    print(\"Sample Conversation:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if \"messages\" in sample:\n",
    "        for msg in sample[\"messages\"][:4]:  # Show first 4 messages\n",
    "            role = msg.get(\"role\", \"unknown\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            print(f\"\\n{role.upper()}:\")\n",
    "            print(content[:500])  # Truncate long messages\n",
    "            if len(content) > 500:\n",
    "                print(\"...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"  Total samples: {len(dataset)}\")\n",
    "    print(f\"  Average messages per sample: {sum(len(s.get('messages', [])) for s in dataset.samples) / len(dataset):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning with SFT\n",
    "\n",
    "Train the model using the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfabric.training import SFTTrainingConfig, LoRAConfig\n",
    "\n",
    "# Configure training\n",
    "training_config = SFTTrainingConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    # LoRA configuration (memory efficient)\n",
    "    lora=LoRAConfig(\n",
    "        enabled=USE_LORA,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_R * 2,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=\"all-linear\",\n",
    "    ),\n",
    "    \n",
    "    # Memory optimization\n",
    "    bf16=True,  # Use bfloat16 mixed precision\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # HuggingFace Hub (optional)\n",
    "    push_to_hub=PUSH_TO_HUB,\n",
    "    hub_model_id=HUB_MODEL_ID if PUSH_TO_HUB else None,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * 4})\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  LoRA: {USE_LORA} (r={LORA_R})\")\n",
    "print(f\"  Mixed precision: bf16\")\n",
    "print(f\"  Gradient checkpointing: True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"This will take some time depending on:\")\n",
    "print(\"  - Dataset size\")\n",
    "print(\"  - Model size\")\n",
    "print(\"  - Number of epochs\")\n",
    "print(\"  - GPU speed\\n\")\n",
    "\n",
    "metrics = pipeline.train(\n",
    "    training_config=training_config,\n",
    "    formatting_func=\"trl_sft_tools\",  # Format for chat-based training\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"\\nFinal metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "pipeline.save_and_upload(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dataset_path=DATASET_PATH,\n",
    "    hf_repo=HUB_MODEL_ID if PUSH_TO_HUB else None,\n",
    ")\n",
    "\n",
    "print(f\"✓ Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"✓ Dataset saved to: {DATASET_PATH}\")\n",
    "if PUSH_TO_HUB:\n",
    "    print(f\"✓ Uploaded to HuggingFace Hub: {HUB_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample prompt\n",
    "test_prompt = \"What is the difference between a list and a tuple in Python?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "# Format using chat template\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(formatted, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Test prompt: {test_prompt}\\n\")\n",
    "print(\"Generating response...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract just the assistant's response\n",
    "if \"assistant\" in response:\n",
    "    response = response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "print(\"Response:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✓ GPU memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Google Colab / Kaggle\n",
    "\n",
    "### Google Colab\n",
    "1. Use a GPU runtime: Runtime → Change runtime type → GPU\n",
    "2. For large models, use Colab Pro for better GPUs (A100, V100)\n",
    "3. Mount Google Drive to save models:\n",
    "   ```python\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')\n",
    "   OUTPUT_DIR = '/content/drive/MyDrive/models/my_model'\n",
    "   ```\n",
    "\n",
    "### Kaggle\n",
    "1. Enable GPU: Settings → Accelerator → GPU\n",
    "2. Kaggle provides 30h/week of GPU time\n",
    "3. Save to `/kaggle/working` or use Kaggle Datasets\n",
    "\n",
    "### Memory Tips\n",
    "- Use smaller models (3B instead of 7B) if you hit OOM errors\n",
    "- Reduce `BATCH_SIZE` to 1 or 2\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- Enable `gradient_checkpointing=True`\n",
    "- Use 4-bit quantization for very large models:\n",
    "  ```python\n",
    "  training_config = SFTTrainingConfig(\n",
    "      ...\n",
    "      quantization=QuantizationConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "      ),\n",
    "  )\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}